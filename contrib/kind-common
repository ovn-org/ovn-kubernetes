if [ "${BASH_SOURCE[0]}" -ef "$0" ]
then
    >&2 echo 'This file contains bash helper functions that are common to'
    >&2 echo 'kind.sh and kind-helm.sh scripts and is meant to be sourced'
    >&2 echo 'by them upon invocation. In other words, it is not useful'
    >&2 echo 'when executed as a standalone script.'
    >&2 echo 'Please source this script, do not execute it!'
    exit 1
fi

ARCH=""
case $(uname -m) in
    x86_64)  ARCH="amd64" ;;
    aarch64) ARCH="arm64"   ;;
esac

if_error_exit() {
    ###########################################################################
    # Description:                                                            #
    # Validate if previous command failed and show an error msg (if provided) #
    #                                                                         #
    # Arguments:                                                              #
    #   $1 - error message if not provided, it will just exit                 #
    ###########################################################################
    if [ "$?" != "0" ]; then
        if [ -n "$1" ]; then
            RED="\e[31m"
            ENDCOLOR="\e[0m"
            echo -e "[ ${RED}FAILED${ENDCOLOR} ] ${1}"
        fi
        exit 1
    fi
}

run_kubectl() {
  local retries=0
  local attempts=10
  while true; do
    if kubectl "$@"; then
      break
    fi

    ((retries += 1))
    if [[ "${retries}" -gt ${attempts} ]]; then
      echo "error: 'kubectl $*' did not succeed, failing"
      exit 1
    fi
    echo "info: waiting for 'kubectl $*' to succeed..."
    sleep 1
  done
}

command_exists() {
  cmd="$1"
  command -v ${cmd} >/dev/null 2>&1
}

detect_apiserver_url() {
  # Detect API_URL used for in-cluster communication
  #
  # Despite OVN run in pod they will only obtain the VIRTUAL apiserver address
  # and since OVN has to provide the connectivity to service
  # it can not be bootstrapped
  #
  # This is the address of the node with the control-plane
  API_URL=$(kind get kubeconfig --internal --name "${KIND_CLUSTER_NAME}" | grep server | awk '{ print $2 }')
}

docker_disable_ipv6() {
  # Docker disables IPv6 globally inside containers except in the eth0 interface.
  # Kind enables IPv6 globally the containers ONLY for dual-stack and IPv6 deployments.
  # Ovnkube-node tries to move all global addresses from the gateway interface to the
  # bridge interface it creates. This breaks on KIND with IPv4 only deployments, because the new
  # internal bridge has IPv6 disable and can't move the IPv6 from the eth0 interface.
  # We can enable IPv6 always in the container, since the docker setup with IPv4 only
  # is not very common.
  KIND_NODES=$(kind_get_nodes)
  for n in $KIND_NODES; do
    $OCI_BIN exec "$n" sysctl --ignore net.ipv6.conf.all.disable_ipv6=0
    $OCI_BIN exec "$n" sysctl --ignore net.ipv6.conf.all.forwarding=1
  done
}

coredns_patch() {
  dns_server="8.8.8.8"
  # No need for ipv6 nameserver for dual stack, it will ask for 
  # A and AAAA records
  if [ "$IP_FAMILY" == "ipv6" ]; then
    dns_server="2001:4860:4860::8888"
  fi

  # Patch CoreDNS to work
  # 1. Github CI doesnÂ´t offer IPv6 connectivity, so CoreDNS should be configured
  # to work in an offline environment:
  # https://github.com/coredns/coredns/issues/2494#issuecomment-457215452
  # 2. Github CI adds following domains to resolv.conf search field:
  # .net.
  # CoreDNS should handle those domains and answer with NXDOMAIN instead of SERVFAIL
  # otherwise pods stops trying to resolve the domain.
  # Get the current config
  original_coredns=$(kubectl get -oyaml -n=kube-system configmap/coredns)
  echo "Original CoreDNS config:"
  echo "${original_coredns}"
  # Patch it
  fixed_coredns=$(
    printf '%s' "${original_coredns}" | sed \
      -e 's/^.*kubernetes cluster\.local/& net/' \
      -e '/^.*upstream$/d' \
      -e '/^.*fallthrough.*$/d' \
      -e 's/^\(.*forward \.\).*$/\1 '"$dns_server"' {/' \
      -e '/^.*loop$/d' \
  )
  echo "Patched CoreDNS config:"
  echo "${fixed_coredns}"
  printf '%s' "${fixed_coredns}" | kubectl apply -f -
}

install_ingress() {
  run_kubectl apply -f "${DIR}/ingress/mandatory.yaml"
  run_kubectl apply -f "${DIR}/ingress/service-nodeport.yaml"
}

METALLB_DIR="/tmp/metallb"
install_metallb() {
  local metallb_version=v0.14.8
  mkdir -p /tmp/metallb
  local builddir
  builddir=$(mktemp -d "${METALLB_DIR}/XXXXXX")

  pushd "${builddir}"
  git clone https://github.com/metallb/metallb.git -b $metallb_version
  cd metallb
  # Use global IP next hops in IPv6
  if  [ "$KIND_IPV6_SUPPORT" == true ]; then
    sed -i '/address-family PROTOCOL unicast/a \
  neighbor NODE0_IP route-map IPV6GLOBAL in\n  neighbor NODE1_IP route-map IPV6GLOBAL in\n  neighbor NODE2_IP route-map IPV6GLOBAL in' dev-env/bgp/frr/bgpd.conf.tmpl
    printf "route-map IPV6GLOBAL permit 10\n set ipv6 next-hop prefer-global" >> dev-env/bgp/frr/bgpd.conf.tmpl
  fi
  pip install -r dev-env/requirements.txt

  local ip_family ipv6_network
  if [ "$KIND_IPV4_SUPPORT" == true ] && [ "$KIND_IPV6_SUPPORT" == true ]; then
    ip_family="dual"
    ipv6_network="--ipv6 --subnet=${METALLB_CLIENT_NET_SUBNET_IPV6}"
  elif  [ "$KIND_IPV6_SUPPORT" == true ]; then
    ip_family="ipv6"
    ipv6_network="--ipv6 --subnet=${METALLB_CLIENT_NET_SUBNET_IPV6}"
  else
    ip_family="ipv4"
    ipv6_network=""
  fi
  # Override GOBIN until https://github.com/metallb/metallb/issues/2218 is fixed.
  GOBIN="" inv dev-env -n ovn -b frr -p bgp -i "${ip_family}"

  docker network create --subnet="${METALLB_CLIENT_NET_SUBNET_IPV4}" ${ipv6_network} --driver bridge clientnet
  docker network connect clientnet frr
  if  [ "$KIND_IPV6_SUPPORT" == true ]; then
    # Enable IPv6 forwarding in FRR
    docker exec frr sysctl -w net.ipv6.conf.all.forwarding=1
  fi
  # Note: this image let's us use it also for creating load balancer backends that can send big packets
  docker run  --cap-add NET_ADMIN --user 0  -d --network clientnet  --rm  --name lbclient  quay.io/itssurya/dev-images:metallb-lbservice
  popd
  delete_metallb_dir

  # The metallb commit https://github.com/metallb/metallb/commit/1a8e52c393d40efd17f28491616f6f9f7790a522
  # removes control plane node from acting as a bgp speaker for service routes.
  # Hence remove node.kubernetes.io/exclude-from-external-load-balancers label from control-plane nodes
  # so that they are also available for advertising bgp routes which are needed for ovnkube's service
  # specific e2e tests.
  MASTER_NODES=$(kind_get_nodes | sort | head -n "${KIND_NUM_MASTER}")
  for n in $MASTER_NODES; do
    kubectl label node "$n" node.kubernetes.io/exclude-from-external-load-balancers-
  done

  kind_network_v4=$(docker inspect -f '{{index .NetworkSettings.Networks "kind" "IPAddress"}}' frr)
  echo "FRR kind network IPv4: ${kind_network_v4}"
  kind_network_v6=$(docker inspect -f '{{index .NetworkSettings.Networks "kind" "GlobalIPv6Address"}}' frr)
  echo "FRR kind network IPv6: ${kind_network_v6}"
  local client_network_v4 client_network_v6
  client_network_v4=$(docker inspect -f '{{index .NetworkSettings.Networks "clientnet" "IPAddress"}}' frr)
  echo "FRR client network IPv4: ${client_network_v4}"
  client_network_v6=$(docker inspect -f '{{index .NetworkSettings.Networks "clientnet" "GlobalIPv6Address"}}' frr)
  echo "FRR client network IPv6: ${client_network_v6}"

  local client_subnets
  client_subnets=$(docker network inspect clientnet -f '{{range .IPAM.Config}}{{.Subnet}}#{{end}}')
  echo "${client_subnets}"
  local client_subnets_v4 client_subnets_v6
  client_subnets_v4=$(echo "${client_subnets}" | cut -d '#' -f 1)
  echo "client subnet IPv4: ${client_subnets_v4}"
  client_subnets_v6=$(echo "${client_subnets}" | cut -d '#' -f 2)
  echo "client subnet IPv6: ${client_subnets_v6}"

  KIND_NODES=$(kind_get_nodes)
  for n in ${KIND_NODES}; do
    if [ "$KIND_IPV4_SUPPORT" == true ]; then
        docker exec "${n}" ip route add "${client_subnets_v4}" via "${kind_network_v4}"
    fi
    if [ "$KIND_IPV6_SUPPORT" == true ]; then
        docker exec "${n}" ip -6 route add "${client_subnets_v6}" via "${kind_network_v6}"
    fi
  done

  # for now, we only run one test with metalLB load balancer for which this
  # one svcVIP (192.168.10.0/fc00:f853:ccd:e799::) is more than enough since at a time we will only
  # have one load balancer service
  if [ "$KIND_IPV4_SUPPORT" == true ]; then
    docker exec lbclient ip route add 192.168.10.0 via "${client_network_v4}" dev eth0
  fi
  if [ "$KIND_IPV6_SUPPORT" == true ]; then
    docker exec lbclient ip -6 route add fc00:f853:ccd:e799:: via "${client_network_v6}" dev eth0
  fi
  sleep 30
}

install_plugins() {
  git clone https://github.com/containernetworking/plugins.git
  pushd plugins
  CGO_ENABLED=0 ./build_linux.sh
  KIND_NODES=$(kind_get_nodes)
  # Opted for not overwritting the existing plugins
  for node in $KIND_NODES; do
    for plugin in bandwidth bridge dhcp dummy firewall host-device ipvlan macvlan sbr static tuning vlan vrf; do
      $OCI_BIN cp ./bin/$plugin $node:/opt/cni/bin/
    done
  done
  popd
  rm -rf plugins
}

destroy_metallb() {
  if docker ps --format '{{.Names}}' | grep -Eq '^lbclient$'; then
      docker stop lbclient
  fi
  if docker ps --format '{{.Names}}' | grep -Eq '^frr$'; then
      docker stop frr
  fi
  if docker network ls --format '{{.Name}}' | grep -q '^clientnet$'; then
      docker network rm clientnet
  fi
  delete_metallb_dir
}

delete_metallb_dir() {
  if ! [ -d "${METALLB_DIR}" ]; then
      return
  fi

  # The build directory will contain read only directories after building. Files cannot be deleted, even by the owner.
  # Therefore, set all dirs to u+rwx.
  find "${METALLB_DIR}" -type d -exec chmod u+rwx "{}" \;
  rm -rf "${METALLB_DIR}"
}

# kubectl_wait_pods will set a total timeout of 300s for IPv4 and 480s for IPv6. It will first wait for all
# DaemonSets to complete with kubectl rollout. This command will block until all pods of the DS are actually up.
# Next, it iterates over all pods with name=ovnkube-db and ovnkube-master and waits for them to post "Ready".
# Last, it will do the same with all pods in the kube-system namespace.
kubectl_wait_pods() {
  # IPv6 cluster seems to take a little longer to come up, so extend the wait time.
  OVN_TIMEOUT=300
  if [ "$KIND_IPV6_SUPPORT" == true ]; then
    OVN_TIMEOUT=480
  fi

  # We will make sure that we timeout all commands at current seconds + the desired timeout.
  endtime=$(( SECONDS + OVN_TIMEOUT ))

  for ds in ovnkube-node ovs-node; do
    timeout=$(calculate_timeout ${endtime})
    echo "Waiting for k8s to launch all ${ds} pods (timeout ${timeout})..."
    kubectl rollout status daemonset -n ovn-kubernetes ${ds} --timeout ${timeout}s
  done

  pods=""
  if [ "$OVN_ENABLE_INTERCONNECT" == true ]; then
    pods="ovnkube-control-plane"
  else
    pods="ovnkube-master ovnkube-db"
  fi
  for name in ${pods}; do
    timeout=$(calculate_timeout ${endtime})
    echo "Waiting for k8s to create ${name} pods (timeout ${timeout})..."
    kubectl wait pods -n ovn-kubernetes -l name=${name} --for condition=Ready --timeout=${timeout}s
  done

  timeout=$(calculate_timeout ${endtime})
  if ! kubectl wait -n kube-system --for=condition=ready pods --all --timeout=${timeout}s ; then
    echo "some pods in the system are not running"
    kubectl get pods -A -o wide || true
    exit 1
  fi
}

# calculate_timeout takes an absolute endtime in seconds (based on bash script runtime, see
# variable $SECONDS) and calculates a relative timeout value. Should the calculated timeout
# be <= 0, return one second.
calculate_timeout() {
  endtime=$1
  timeout=$(( endtime - SECONDS ))
  if [ ${timeout} -le 0 ]; then
      timeout=1
  fi
  echo ${timeout}
}

sleep_until_pods_settle() {
  echo "Pods are all up, allowing things settle for 30 seconds..."
  sleep 30
}

is_nested_virt_enabled() {
    local kvm_nested="unknown"
    if [ -f "/sys/module/kvm_intel/parameters/nested" ]; then
        kvm_nested=$( cat /sys/module/kvm_intel/parameters/nested )
    elif [ -f "/sys/module/kvm_amd/parameters/nested" ]; then
        kvm_nested=$( cat /sys/module/kvm_amd/parameters/nested )
    fi
    [ "$kvm_nested" == "1" ] || [ "$kvm_nested" == "Y" ] || [ "$kvm_nested" == "y" ]
}

install_kubevirt() {
    # possible values:
    # stable - install newest stable (default)
    # vX.Y.Z - install specific stable (i.e v1.3.1)
    # nightly - install newest nightly
    # nightly tag - install specific nightly (i.e 20240910)
    KUBEVIRT_VERSION=${KUBEVIRT_VERSION:-"stable"}

    for node in $(kubectl get node --no-headers  -o custom-columns=":metadata.name"); do
        $OCI_BIN exec -t $node bash -c "echo 'fs.inotify.max_user_watches=1048576' >> /etc/sysctl.conf"
        $OCI_BIN exec -t $node bash -c "echo 'fs.inotify.max_user_instances=512' >> /etc/sysctl.conf"
        $OCI_BIN exec -i $node bash -c "sysctl -p /etc/sysctl.conf"
        if [[ "${node}" =~ worker ]]; then
            kubectl label nodes $node node-role.kubernetes.io/worker="" --overwrite=true
        fi
    done

    if [ "$(kubectl get kubevirts -n kubevirt kubevirt -ojsonpath='{.status.phase}')" != "Deployed" ]; then
      local kubevirt_release_url=$(get_kubevirt_release_url "$KUBEVIRT_VERSION")
      echo "Deploying Kubevirt from $kubevirt_release_url"
      kubectl apply -f "${kubevirt_release_url}/kubevirt-operator.yaml"
      kubectl apply -f "${kubevirt_release_url}/kubevirt-cr.yaml"
      if ! is_nested_virt_enabled; then
        kubectl -n kubevirt patch kubevirt kubevirt --type=merge --patch '{"spec":{"configuration":{"developerConfiguration":{"useEmulation":true}}}}'
      fi
      kubectl -n kubevirt patch kubevirt kubevirt --type=merge --patch '{"spec":{"configuration":{"virtualMachineOptions":{"disableSerialConsoleLog":{}}}}}'
    fi
    if ! kubectl wait -n kubevirt kv kubevirt --for condition=Available --timeout 15m; then
        kubectl get pod -n kubevirt -l || true
        kubectl describe pod -n kubevirt -l || true
        for p in $(kubectl get pod -n kubevirt -l -o name |sed "s#pod/##"); do
            kubectl logs -p --all-containers=true -n kubevirt $p || true
            kubectl logs --all-containers=true -n kubevirt $p || true
        done
    fi

    kubectl -n kubevirt patch kubevirt kubevirt --type=json --patch '[{"op":"add","path":"/spec/configuration/developerConfiguration","value":{"featureGates":[]}},{"op":"add","path":"/spec/configuration/developerConfiguration/featureGates/-","value":"NetworkBindingPlugins"},{"op":"add","path":"/spec/configuration/developerConfiguration/featureGates/-","value":"DynamicPodInterfaceNaming"}]'

    local kubevirt_stable_release_url=$(get_kubevirt_release_url "stable")
    local passt_binding_image="quay.io/kubevirt/network-passt-binding:${kubevirt_stable_release_url##*/}"
    kubectl -n kubevirt patch kubevirt kubevirt --type=json --patch '[{"op":"add","path":"/spec/configuration/network","value":{}},{"op":"add","path":"/spec/configuration/network/binding","value":{"passt":{"computeResourceOverhead":{"requests":{"memory":"500Mi"}},"migration":{"method":"link-refresh"},"networkAttachmentDefinition":"default/primary-udn-kubevirt-binding","sidecarImage":"'"${passt_binding_image}"'"},"managedTap":{"domainAttachmentType":"managedTap","migration":{}}}}]'
    
    if [ ! -d "./bin" ]
    then
        mkdir -p ./bin
        if_error_exit "Failed to create bin dir!"
    fi

    if [[ "$OSTYPE" == "linux-gnu" ]]; then
        OS_TYPE="linux"
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        OS_TYPE="darwin"
    fi

    pushd ./bin
       if [ ! -f ./virtctl ]; then
           kubevirt_stable_release_url=$(get_kubevirt_release_url "stable")
           cli_name="virtctl-${kubevirt_stable_release_url##*/}-${OS_TYPE}-${ARCH}"
           curl -LO "${kubevirt_stable_release_url}/${cli_name}"
           mv ${cli_name} virtctl
           if_error_exit "Failed to download virtctl!"
       fi
    popd

    chmod +x ./bin/virtctl
}

install_cert_manager() {
  local cert_manager_version="v1.14.4"
  echo "Installing cert-manager ..."
  manifest="https://github.com/cert-manager/cert-manager/releases/download/${cert_manager_version}/cert-manager.yaml"
  run_kubectl apply -f "$manifest"
}

install_kubevirt_ipam_controller() {
  echo "Installing KubeVirt IPAM controller manager ..."
  manifest="https://raw.githubusercontent.com/kubevirt/ipam-extensions/main/dist/install.yaml"
  run_kubectl apply -f "$manifest"
  kubectl wait -n kubevirt-ipam-controller-system deployment kubevirt-ipam-controller-manager --for condition=Available --timeout 2m
}

install_multus() {
  local version="v4.1.3"
  echo "Installing multus-cni $version daemonset ..."
  wget -qO- "https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/${version}/deployments/multus-daemonset.yml" |\
    sed -e "s|multus-cni:snapshot|multus-cni:${version}|g" |\
    run_kubectl apply -f -
}

install_mpolicy_crd() {
  echo "Installing multi-network-policy CRD ..."
  mpolicy_manifest="https://raw.githubusercontent.com/k8snetworkplumbingwg/multi-networkpolicy/master/scheme.yml"
  run_kubectl apply -f "$mpolicy_manifest"
}

install_ipamclaim_crd() {
  echo "Installing IPAMClaim CRD ..."
  ipamclaims_manifest="https://raw.githubusercontent.com/k8snetworkplumbingwg/ipamclaims/v0.4.0-alpha/artifacts/k8s.cni.cncf.io_ipamclaims.yaml"
  run_kubectl apply -f "$ipamclaims_manifest"
}

docker_create_second_disconnected_interface() {
  echo "adding second interfaces to nodes"
  local bridge_name="${1:-kindexgw}"
  echo "bridge: $bridge_name"

  if [ "${OCI_BIN}" = "podman" ]; then
    # docker and podman do different things with the --internal parameter:
    # - docker installs iptables rules to drop traffic on a different subnet
    #   than the bridge and we don't want that.
    # - podman does not set the bridge as default gateway and we want that.
    # So we need it with podman but not with docker. Neither allows us to create
    # a bridge network without IPAM which would be ideal, so perhaps the best
    # option would be a manual setup.
    local podman_params="--internal"
  fi

  # Create the network without subnets; ignore if already exists.
  "$OCI_BIN" network create --driver=bridge ${podman_params-} "$bridge_name" || true

  KIND_NODES=$(kind_get_nodes)
  for n in $KIND_NODES; do
    "$OCI_BIN" network connect "$bridge_name" "$n" || true
  done
}

enable_multi_net() {
  install_multus
  install_mpolicy_crd
  install_ipamclaim_crd
  docker_create_second_disconnected_interface "underlay"  # localnet scenarios require an extra interface
}

kind_get_nodes() {
  kind get nodes --name "${KIND_CLUSTER_NAME}" | grep -v external-load-balancer
}

set_dnsnameresolver_images() {
  if [ "$KIND_LOCAL_REGISTRY" == true ];then
    COREDNS_WITH_OCP_DNSNAMERESOLVER="localhost:5000/coredns-with-ocp-dnsnameresolver:latest"
    DNSNAMERESOLVER_OPERATOR="localhost:5000/dnsnameresolver-operator:latest"
  else
    COREDNS_WITH_OCP_DNSNAMERESOLVER="localhost/coredns-with-ocp-dnsnameresolver:dev"
    DNSNAMERESOLVER_OPERATOR="localhost/dnsnameresolver-operator:dev"
  fi
}

# build_image accepts three arguments. The first argument is the absolute path to the directory
# which contains the Dockerfile. The second argument is the image name along with the tag. The
# third argument is the name of the Dockerfile to use for building the image. 
build_image() {
  pushd ${1}
  $OCI_BIN build -t "${2}" -f ${3} .

  # store in local registry
  if [ "$KIND_LOCAL_REGISTRY" == true ];then
    echo "Pushing built image (${2}) to local $OCI_BIN registry"
    $OCI_BIN push "${2}"
  fi
  popd
}

build_dnsnameresolver_images() {
  set_dnsnameresolver_images
  rm -rf /tmp/coredns-ocp-dnsnameresolver
  git clone https://github.com/openshift/coredns-ocp-dnsnameresolver.git /tmp/coredns-ocp-dnsnameresolver
  pushd /tmp/coredns-ocp-dnsnameresolver
  git checkout 850cb5757982d368195744d3565c1565a7f0d43a
  popd
 
  build_image /tmp/coredns-ocp-dnsnameresolver ${COREDNS_WITH_OCP_DNSNAMERESOLVER} Dockerfile.upstream

  build_image /tmp/coredns-ocp-dnsnameresolver/operator ${DNSNAMERESOLVER_OPERATOR} Dockerfile
}

# install_image accepts the image name along with the tag as an argument and installs it.
install_image() {
  # If local registry is being used push image there for consumption by kind cluster
  if [ "$KIND_LOCAL_REGISTRY" == true ]; then
    echo "${1} should already be avaliable in local registry, not loading"
  else
    if [ "$OCI_BIN" == "podman" ]; then
      # podman: cf https://github.com/kubernetes-sigs/kind/issues/2027
      rm -f /tmp/image.tar
      podman save -o /tmp/image.tar "${1}"
      kind load image-archive /tmp/image.tar --name "${KIND_CLUSTER_NAME}"
    else
      kind load docker-image "${1}" --name "${KIND_CLUSTER_NAME}"
    fi
  fi
}

install_dnsnameresolver_images() {
  install_image ${COREDNS_WITH_OCP_DNSNAMERESOLVER}
  install_image ${DNSNAMERESOLVER_OPERATOR}
}

install_dnsnameresolver_operator() {
  pushd /tmp/coredns-ocp-dnsnameresolver/operator
  
  # Before installing DNSNameResolver operator, update the args so that the operator
  # is configured with the correct values.
  sed -i -e 's/^\(.*--coredns-namespace=\).*/\1kube-system/' \
    -e 's/^\(.*--coredns-service-name=\).*/\1kube-dns/' \
    -e 's/^\(.*--dns-name-resolver-namespace=\).*/\1ovn-kubernetes/' \
    -e 's/^\(.*--coredns-port=\).*/\153/' config/default/manager_auth_proxy_patch.yaml

  make install
  make deploy IMG=${DNSNAMERESOLVER_OPERATOR}
  popd
}

update_clusterrole_coredns() {
  original_clusterrole=$(kubectl get clusterrole system:coredns -oyaml)
  echo "Original CoreDNS clusterrole:"
  echo "${original_clusterrole}"
  additional_permissions=$(printf '%s' '
- apiGroups:
  - network.openshift.io
  resources:
  - dnsnameresolvers
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - network.openshift.io
  resources:
  - dnsnameresolvers/status
  verbs:
  - update
  - get
  - patch
')
  updated_clusterrole="${original_clusterrole}${additional_permissions}"
  echo "Patched CoreDNS clusterrole:"
  echo "${updated_clusterrole}"
  printf '%s' "${updated_clusterrole}" | kubectl apply -f -
}

add_ocp_dnsnameresolver_to_coredns_config() {
  original_corefile=$(kubectl get -n=kube-system configmap/coredns -o=jsonpath="{.data['Corefile']}")
  if ! grep -wq "ocp_dnsnameresolver" <<< ${original_corefile}; then
    echo "Original CoreDNS Corefile:"
    echo "${original_corefile}"
    updated_corefile=$(
      printf '%s' "${original_corefile}" | sed -e 's/^\(.*\)\(forward.*\)/\1ocp_dnsnameresolver {\n\1   namespaces ovn-kubernetes\n\1}\n\1\2/'
    )
    echo "Patched CoreDNS Corefile:"
    echo "${updated_corefile}"
    printf '%s' "${updated_corefile}" > /tmp/Corefile.json
    updated_coredns=$(kubectl create configmap coredns -n=kube-system --from-file=Corefile=/tmp/Corefile.json -oyaml --dry-run=client)
    echo "Patched CoreDNS config:"
    echo "${updated_coredns}"
    printf '%s' "${updated_coredns}" | kubectl apply -f -
  fi
}

update_coredns_deployment_image() {
  kubectl -n=kube-system set image deploy/coredns coredns=${COREDNS_WITH_OCP_DNSNAMERESOLVER}
}

# kubectl_wait_dnsnameresolver_pods will set a total timeout of 60s and wait for the pods
# related to the dns name resolver feature to become "Ready".
kubectl_wait_dnsnameresolver_pods() {
  TIMEOUT=60

  # We will make sure that we timeout all commands at current seconds + the desired timeout.
  endtime=$(( SECONDS + TIMEOUT ))

  timeout=$(calculate_timeout ${endtime})
  echo "Waiting for pods in dnsnameresolver-operator namespace to become ready (timeout ${timeout})..."
  kubectl wait -n dnsnameresolver-operator --for=condition=ready pods --all --timeout=${timeout}s
}

deploy_kubevirt_binding() {
  cat <<EOF | run_kubectl apply -f -
---
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: primary-udn-kubevirt-binding
  namespace: default
spec:
  config: '{
  "cniVersion": "1.0.0",
  "name": "primary-udn-kubevirt-binding",
  "plugins": [
    {
      "type": "network-passt-binding"
    }
  ]
}'
EOF
}

deploy_passt_binary() {
  echo "Installing passt-binding-cni-ds ..."
  local manifest="https://raw.githubusercontent.com/kubevirt/ipam-extensions/main/passt/passt-binding-cni-ds.yaml"
  run_kubectl apply -f "$manifest"

  run_kubectl rollout status -n kube-system daemonset/passt-binding-cni --timeout 2m
}

get_kubevirt_release_url() {
    local VERSION="$1"

    local kubevirt_version
    local kubevirt_release_url

    if [[ "$VERSION" == "stable" ]]; then
        kubevirt_version=$(curl -sL https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt)
        kubevirt_release_url="https://github.com/kubevirt/kubevirt/releases/download/${kubevirt_version}"
    elif [[ "$VERSION" == v* ]]; then
        kubevirt_version="$VERSION"
        kubevirt_release_url="https://github.com/kubevirt/kubevirt/releases/download/${kubevirt_version}"
    elif [[ "$VERSION" == "nightly" ]]; then
        kubevirt_version=$(curl -sL https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/latest)
        kubevirt_release_url="https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${kubevirt_version}"
    elif [[ "$VERSION" =~ ^[0-9]{8}$ ]]; then
        kubevirt_version="$VERSION"
        kubevirt_release_url="https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${kubevirt_version}"
    else
        echo "Unsupported KUBEVIRT_VERSION value $VERSION (use either stable, vX.Y.Z, nightly or nightly tag)"
        exit 1
    fi

    echo "$kubevirt_release_url"
}

readonly FRR_K8S_VERSION=v0.0.14
readonly FRR_TMP_DIR=$(mktemp -d -u)

clone_frr() {
  [ -d "$FRR_TMP_DIR" ] || {
    mkdir -p "$FRR_TMP_DIR" && trap 'rm -rf $FRR_TMP_DIR' EXIT
    pushd "$FRR_TMP_DIR" || exit 1
    git clone --depth 1 --branch $FRR_K8S_VERSION https://github.com/metallb/frr-k8s
    popd || exit 1
  }
}

deploy_frr_external_container() {
  echo "Deploying FRR external container ..."
  clone_frr
 
  pushd "$FRR_TMP_DIR" || exit 1
  run_kubectl apply -f frr-k8s/charts/frr-k8s/charts/crds/templates/frrk8s.metallb.io_frrconfigurations.yaml
  popd || exit 1
 
  # apply the demo which will deploy an external FRR container that the cluster
  # can peer with acting as BGP (reflector) external gateway
  pushd "${FRR_TMP_DIR}"/frr-k8s/hack/demo || exit 1
  # modify config template to configure neighbors as route reflector clients
  sed -i '/remote-as 64512/a \ neighbor {{ . }} route-reflector-client' frr/frr.conf.tmpl
  ./demo.sh
  popd || exit 1

  # this container will act as the gateway for the cluster and will masquerade
  # towards the external world
  $OCI_BIN exec frr iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
  # set default route
  FRR_IP=$($OCI_BIN inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}" frr)
  KIND_NODES=$(kind_get_nodes)
  for n in $KIND_NODES; do
    $OCI_BIN exec "$n" ip route replace default via "$FRR_IP"
  done
}

install_ffr_k8s() {
  echo "Installing frr-k8s ..."
  clone_frr

  # apply frr-k8s
  kubectl apply -f "${FRR_TMP_DIR}"/frr-k8s/config/all-in-one/frr-k8s.yaml
  kubectl wait -n frr-k8s-system deployment frr-k8s-webhook-server --for condition=Available --timeout 2m
  kubectl rollout status -n frr-k8s-system daemonset frr-k8s-daemon --timeout 2m

  # apply a BGP peer configration with the external gateway that does not
  # exchange routes
  pushd "${FRR_TMP_DIR}"/frr-k8s/hack/demo/configs || exit 1
  sed 's/all$/filtered/g' receive_all.yaml > receive_filtered.yaml
  kubectl apply -f receive_filtered.yaml
  popd || exit 1

  rm -rf "${FRR_TMP_DIR}"
}
